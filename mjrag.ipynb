{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案\n",
    "file_path = r\"C:\\Users\\mindy\\桌面\\張茗溱.pdf\"\n",
    "# file_path = r\"E:\\腸易激.pdf\"\n",
    "loader = file_path.endswith(\".pdf\") and PyPDFLoader(file_path) or TextLoader(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇 splitter 並將文字切分成多個 chunk \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) \n",
    "texts = loader.load_and_split(splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mindy\\AppData\\Local\\Temp\\ipykernel_6596\\3861624617.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"shibing624/text2vec-base-chinese\")\n",
      "c:\\Users\\mindy\\anaconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 建立本地 db\n",
    "# 建立本地 db\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"shibing624/text2vec-base-chinese\")\n",
    "db = Chroma.from_documents(\n",
    "    texts, \n",
    "    embeddings, \n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\\Users\\mindy\\.cache\\huggingface\\hub\\models--taide--TAIDE-LX-7B-Chat-4bit\\snapshots\\9063ed154144775841f3953b69b534c6e2d564d8\\.\\taide-7b-a.2-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,56064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,56064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,56064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 47\n",
      "llm_load_vocab: token to piece cache size = 0.2613 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 56064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.94 B\n",
      "llm_load_print_meta: model size       = 3.93 GiB (4.86 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<pad>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4021.23 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 4096\n",
      "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.21 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}\\n    {% set loop_messages = messages[1:] %}\\n    {% set system_message = messages[0]['content'] %}\\n{% else %}\\n    {% set loop_messages = messages %}\\n    {% set system_message = false %}\\n{% endif %}\\n{% for message in loop_messages %}\\n    {% if loop.index0 == 0 and system_message != false %}\\n        {% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}\\n    {% else %}\\n        {% set content = message['content'] %}\\n    {% endif %}\\n    {% if message['role'] == 'user' %}\\n        {{- bos_token + '[INST] ' + content.strip() + ' [/INST]' -}}\\n    {% elif message['role'] == 'system' %}\\n        {{- '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' -}}\\n    {% elif message['role'] == 'assistant' %}\\n        {{- ' '  + content.strip() + ' ' + eos_token -}}\\n    {% endif %}\\n{% endfor %}\\n\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}\n",
      "    {% set loop_messages = messages[1:] %}\n",
      "    {% set system_message = messages[0]['content'] %}\n",
      "{% else %}\n",
      "    {% set loop_messages = messages %}\n",
      "    {% set system_message = false %}\n",
      "{% endif %}\n",
      "{% for message in loop_messages %}\n",
      "    {% if loop.index0 == 0 and system_message != false %}\n",
      "        {% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}\n",
      "    {% else %}\n",
      "        {% set content = message['content'] %}\n",
      "    {% endif %}\n",
      "    {% if message['role'] == 'user' %}\n",
      "        {{- bos_token + '[INST] ' + content.strip() + ' [/INST]' -}}\n",
      "    {% elif message['role'] == 'system' %}\n",
      "        {{- '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' -}}\n",
      "    {% elif message['role'] == 'assistant' %}\n",
      "        {{- ' '  + content.strip() + ' ' + eos_token -}}\n",
      "    {% endif %}\n",
      "{% endfor %}\n",
      "\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# Setup the pipeline\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"taide/TAIDE-LX-7B-Chat-4bit\",\n",
    "\tfilename=\"taide-7b-a.2-q4_k_m.gguf\",\n",
    "    n_ctx=4096,  # 增加上下文長度，預設通常是 512 或 2048\n",
    "    max_tokens=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mindy\\AppData\\Local\\Temp\\ipykernel_6596\\3192943851.py:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "根據以下檢索資料，請提供一個的回答，並補充必要的背景信息和實例。請確保回答全面且深入。\n",
    "資料：\n",
    "{context}\n",
    "問題：\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "question='張茗溱有哪些實務專案經驗'\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "context = \"\\n\".join([doc.page_content[:1000] for doc in retrieved_docs])  \n",
    "\n",
    "response = llm(f\"{prompt.format(context=context, question=question)}\", max_tokens=2048, temperature=0.8, stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mindy\\anaconda3\\envs\\rag\\python.exe\n",
      "C:\\Users\\mindy\\anaconda3\\python.exe\n",
      "C:\\Users\\mindy\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n",
      "Package                                  Version\n",
      "---------------------------------------- ---------------\n",
      "accelerate                               1.2.1\n",
      "aiofiles                                 24.1.0\n",
      "aiohappyeyeballs                         2.4.4\n",
      "aiohttp                                  3.11.11\n",
      "aiosignal                                1.3.2\n",
      "annotated-types                          0.7.0\n",
      "anyio                                    4.7.0\n",
      "asgiref                                  3.8.1\n",
      "asttokens                                3.0.0\n",
      "async-timeout                            4.0.3\n",
      "attrs                                    24.3.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.2.1\n",
      "beautifulsoup4                           4.12.3\n",
      "build                                    1.2.2.post1\n",
      "cachetools                               5.5.0\n",
      "certifi                                  2024.12.14\n",
      "cffi                                     1.17.1\n",
      "chardet                                  5.2.0\n",
      "charset-normalizer                       3.4.0\n",
      "chroma-hnswlib                           0.7.6\n",
      "chromadb                                 0.5.23\n",
      "click                                    8.1.7\n",
      "colorama                                 0.4.6\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.2\n",
      "contourpy                                1.3.0\n",
      "cryptography                             44.0.0\n",
      "cycler                                   0.12.1\n",
      "dataclasses-json                         0.6.7\n",
      "datasets                                 3.2.0\n",
      "dateparser                               1.2.0\n",
      "debugpy                                  1.8.11\n",
      "decorator                                5.1.1\n",
      "Deprecated                               1.2.15\n",
      "dill                                     0.3.8\n",
      "dirtyjson                                1.0.8\n",
      "diskcache                                5.6.3\n",
      "distro                                   1.9.0\n",
      "docarray                                 0.32.1\n",
      "durationpy                               0.9\n",
      "emoji                                    2.14.0\n",
      "eval_type_backport                       0.2.2\n",
      "exceptiongroup                           1.2.2\n",
      "executing                                2.1.0\n",
      "faiss-cpu                                1.9.0.post1\n",
      "fastapi                                  0.115.6\n",
      "filelock                                 3.16.1\n",
      "filetype                                 1.2.0\n",
      "flatbuffers                              24.3.25\n",
      "fonttools                                4.55.3\n",
      "frozenlist                               1.5.0\n",
      "fsspec                                   2024.9.0\n",
      "google-ai-generativelanguage             0.6.4\n",
      "google-api-core                          2.24.0\n",
      "google-api-python-client                 2.156.0\n",
      "google-auth                              2.37.0\n",
      "google-auth-httplib2                     0.2.0\n",
      "google-generativeai                      0.5.4\n",
      "googleapis-common-protos                 1.66.0\n",
      "GoogleNews                               1.6.15\n",
      "grandalf                                 0.8\n",
      "greenlet                                 3.1.1\n",
      "grpcio                                   1.68.1\n",
      "grpcio-status                            1.62.3\n",
      "h11                                      0.14.0\n",
      "html5lib                                 1.1\n",
      "httpcore                                 1.0.7\n",
      "httplib2                                 0.22.0\n",
      "httptools                                0.6.4\n",
      "httpx                                    0.28.1\n",
      "httpx-sse                                0.4.0\n",
      "huggingface-hub                          0.27.0\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.10\n",
      "importlib_metadata                       8.5.0\n",
      "importlib_resources                      6.4.5\n",
      "iopath                                   0.1.10\n",
      "ipykernel                                6.29.5\n",
      "ipython                                  8.18.1\n",
      "jedi                                     0.19.2\n",
      "jieba                                    0.42.1\n",
      "Jinja2                                   3.1.4\n",
      "jiter                                    0.8.2\n",
      "joblib                                   1.4.2\n",
      "jsonpatch                                1.33\n",
      "jsonpath-python                          1.0.6\n",
      "jsonpointer                              3.0.0\n",
      "jupyter_client                           8.6.3\n",
      "jupyter_core                             5.7.2\n",
      "kiwisolver                               1.4.7\n",
      "kubernetes                               31.0.0\n",
      "langchain                                0.3.13\n",
      "langchain-chroma                         0.2.0\n",
      "langchain-community                      0.3.13\n",
      "langchain-core                           0.3.28\n",
      "langchain-huggingface                    0.1.2\n",
      "langchain-openai                         0.2.14\n",
      "langchain-text-splitters                 0.3.4\n",
      "langdetect                               1.0.9\n",
      "langgraph                                0.2.60\n",
      "langgraph-checkpoint                     2.0.9\n",
      "langgraph-sdk                            0.1.48\n",
      "langsmith                                0.2.4\n",
      "layoutparser                             0.3.4\n",
      "llama-cloud                              0.1.7\n",
      "llama_cpp_python                         0.3.5\n",
      "llama-index                              0.12.8\n",
      "llama-index-agent-openai                 0.4.1\n",
      "llama-index-cli                          0.4.0\n",
      "llama-index-core                         0.12.9\n",
      "llama-index-embeddings-gemini            0.3.0\n",
      "llama-index-embeddings-openai            0.3.1\n",
      "llama-index-indices-managed-llama-cloud  0.6.3\n",
      "llama-index-llms-gemini                  0.4.2\n",
      "llama-index-llms-openai                  0.3.12\n",
      "llama-index-multi-modal-llms-openai      0.4.1\n",
      "llama-index-program-openai               0.3.1\n",
      "llama-index-question-gen-openai          0.3.0\n",
      "llama-index-readers-file                 0.4.1\n",
      "llama-index-readers-llama-parse          0.4.0\n",
      "llama-parse                              0.5.19\n",
      "loguru                                   0.7.3\n",
      "lxml                                     5.3.0\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               3.0.2\n",
      "marshmallow                              3.23.2\n",
      "matplotlib                               3.9.4\n",
      "matplotlib-inline                        0.1.7\n",
      "mdurl                                    0.1.2\n",
      "mmh3                                     5.0.1\n",
      "monotonic                                1.6\n",
      "mpmath                                   1.3.0\n",
      "msgpack                                  1.1.0\n",
      "multidict                                6.1.0\n",
      "multiprocess                             0.70.16\n",
      "mypy-extensions                          1.0.0\n",
      "nest_asyncio                             1.6.0\n",
      "networkx                                 3.2.1\n",
      "nltk                                     3.9.1\n",
      "numpy                                    1.26.4\n",
      "oauthlib                                 3.2.2\n",
      "olefile                                  0.47\n",
      "onnx                                     1.17.0\n",
      "onnxruntime                              1.19.2\n",
      "openai                                   1.58.1\n",
      "opencv-python                            4.10.0.84\n",
      "opentelemetry-api                        1.29.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.29.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.29.0\n",
      "opentelemetry-instrumentation            0.50b0\n",
      "opentelemetry-instrumentation-asgi       0.50b0\n",
      "opentelemetry-instrumentation-fastapi    0.50b0\n",
      "opentelemetry-proto                      1.29.0\n",
      "opentelemetry-sdk                        1.29.0\n",
      "opentelemetry-semantic-conventions       0.50b0\n",
      "opentelemetry-util-http                  0.50b0\n",
      "orjson                                   3.10.12\n",
      "overrides                                7.7.0\n",
      "packaging                                24.2\n",
      "pandas                                   2.2.3\n",
      "parso                                    0.8.4\n",
      "pdf2image                                1.17.0\n",
      "pdfminer.six                             20231228\n",
      "pdfplumber                               0.11.4\n",
      "pi_heif                                  0.21.0\n",
      "pickleshare                              0.7.5\n",
      "pillow                                   10.4.0\n",
      "pinecone-client                          5.0.1\n",
      "pinecone-plugin-inference                1.1.0\n",
      "pinecone-plugin-interface                0.0.7\n",
      "pip                                      24.2\n",
      "platformdirs                             4.3.6\n",
      "portalocker                              3.0.0\n",
      "posthog                                  3.7.4\n",
      "prompt_toolkit                           3.0.48\n",
      "propcache                                0.2.1\n",
      "proto-plus                               1.25.0\n",
      "protobuf                                 5.29.3\n",
      "psutil                                   6.1.0\n",
      "pure_eval                                0.2.3\n",
      "pyarrow                                  18.1.0\n",
      "pyasn1                                   0.6.1\n",
      "pyasn1_modules                           0.4.1\n",
      "pycparser                                2.22\n",
      "pydantic                                 2.10.4\n",
      "pydantic_core                            2.27.2\n",
      "pydantic-settings                        2.7.0\n",
      "Pygments                                 2.18.0\n",
      "PyMuPDF                                  1.25.1\n",
      "pyparsing                                3.2.0\n",
      "pypdf                                    5.1.0\n",
      "PyPDF2                                   3.0.1\n",
      "pypdfium2                                4.30.1\n",
      "PyPika                                   0.48.9\n",
      "pyproject_hooks                          1.2.0\n",
      "pyreadline3                              3.5.4\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.0.1\n",
      "python-iso639                            2024.10.22\n",
      "python-magic                             0.4.27\n",
      "python-multipart                         0.0.20\n",
      "python-oxmsg                             0.0.1\n",
      "pytz                                     2024.2\n",
      "pywin32                                  307\n",
      "PyYAML                                   6.0.2\n",
      "pyzmq                                    26.2.0\n",
      "RapidFuzz                                3.11.0\n",
      "regex                                    2024.11.6\n",
      "requests                                 2.32.3\n",
      "requests-oauthlib                        2.0.0\n",
      "requests-toolbelt                        1.0.0\n",
      "rich                                     13.9.4\n",
      "rsa                                      4.9\n",
      "safetensors                              0.4.5\n",
      "scikit-learn                             1.6.0\n",
      "scipy                                    1.13.1\n",
      "sentence-transformers                    3.3.1\n",
      "setuptools                               75.1.0\n",
      "shellingham                              1.5.4\n",
      "six                                      1.17.0\n",
      "sniffio                                  1.3.1\n",
      "soupsieve                                2.6\n",
      "SQLAlchemy                               2.0.36\n",
      "stack_data                               0.6.3\n",
      "starlette                                0.41.3\n",
      "striprtf                                 0.0.26\n",
      "sympy                                    1.13.1\n",
      "tenacity                                 9.0.0\n",
      "text2vec                                 1.3.2\n",
      "threadpoolctl                            3.5.0\n",
      "tiktoken                                 0.8.0\n",
      "timm                                     1.0.12\n",
      "tokenizers                               0.20.3\n",
      "tomli                                    2.2.1\n",
      "torch                                    2.5.1\n",
      "torchvision                              0.20.1\n",
      "tornado                                  6.4.2\n",
      "tqdm                                     4.67.1\n",
      "traitlets                                5.14.3\n",
      "transformers                             4.46.3\n",
      "typer                                    0.15.1\n",
      "types-requests                           2.32.0.20241016\n",
      "typing_extensions                        4.12.2\n",
      "typing-inspect                           0.9.0\n",
      "tzdata                                   2024.2\n",
      "tzlocal                                  5.2\n",
      "unstructured                             0.16.11\n",
      "unstructured-client                      0.28.1\n",
      "unstructured-inference                   0.8.1\n",
      "uritemplate                              4.1.1\n",
      "urllib3                                  2.2.3\n",
      "uvicorn                                  0.34.0\n",
      "watchfiles                               1.0.3\n",
      "wcwidth                                  0.2.13\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.8.0\n",
      "websockets                               14.1\n",
      "wheel                                    0.44.0\n",
      "win32_setctime                           1.2.0\n",
      "wrapt                                    1.17.0\n",
      "xxhash                                   3.5.0\n",
      "yarl                                     1.18.3\n",
      "zipp                                     3.21.0\n"
     ]
    }
   ],
   "source": [
    "!where python\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'C:\\\\Users\\\\mindy\\\\桌面\\\\張茗溱.pdf'}, page_content='張茗溱，畢業於國立東華大學資訊工程研究所，並持有國立嘉義大學應用數學\\n系的學士學位。他擁有紮實的數學與資訊工程基礎，特別專注於機器學習與深\\n度學習領域，並且在學術研究與實務專案中展現了優異的能力。 \\n在碩士研究期間，張茗溱針對混淆資料的區分與再學習提出了一系列創新的方\\n法，不僅提升了模型在 CIFAR-10 與 CIFAR-100 數據集上的準確率，還解決了\\n資料集分布不均問題，為機器學習模型的設計與應用提供了新的思路。透過自\\n定義的深度學習模型架構，他熟練掌握了 PyTorch 的使用，並展現了規劃實\\n驗、分析結果以及有效解決問題的能力。此外，他對數據處理技術如 Pandas、\\nNumPy，以及機器學習工具如 Scikit-learn、XGBoost、LightGBM 等也有深入了\\n解。 \\n張茗溱在學士期間即展現出卓越的創新能力，參與了多項具有實用性的研究專\\n題。他的畢業專題研究「利用 AI 預測番茄成熟度」中，結合了 K-means 和 \\nSVM 技術，成功解決了數據缺失處理的挑戰，並榮獲嘉義大學理工學院創意專\\n題競賽的佳作。在學術之外，他亦積極投入實務專案，如與同學共同開發了一')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
